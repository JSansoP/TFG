{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "\n",
    "CUDA = True\n",
    "DATA_PATH = 'data'\n",
    "OUT_PATH = 'out'\n",
    "BATCH_SIZE = 256\n",
    "IMAGE_CHANNELS = 1\n",
    "IMAGE_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "REAL_LABEL = 1\n",
    "FAKE_LABEL = 0\n",
    "LEARNING_RATE = 0.0002\n",
    "SEED = 1\n",
    "RANDOM_DIM = 100\n",
    "G_HIDDEN = 64\n",
    "D_HIDDEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dset.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True . Pytorch version:  1.13.0\n",
      "Seed: 1\n"
     ]
    }
   ],
   "source": [
    "#If output folder doesn't exist, create it. Else, delete all files in it.\n",
    "if not os.path.exists(OUT_PATH):\n",
    "    os.makedirs(OUT_PATH)\n",
    "else:\n",
    "    for f in os.listdir(OUT_PATH):\n",
    "        os.remove(os.path.join(OUT_PATH, f))\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "print(\"CUDA Available: \", CUDA, \". Pytorch version: \", torch.__version__)\n",
    "print(\"Seed:\", SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #1st layer\n",
    "        #input: 1x1x100\n",
    "        #output: 4x4x512 bc kernel size is 4 and stride is 1,\n",
    "        # so output size is (input size - 1) * stride + kernel size.\n",
    "        # (1 - 1) * 1 + 4 = 4.\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(RANDOM_DIM, G_HIDDEN * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #2nd layer\n",
    "        #input: 4x4x512\n",
    "        #output: 8x8x256\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #3rd layer\n",
    "        #input: 8x8x256\n",
    "        #output: 16x16x128\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #4th layer\n",
    "        #input: 16x16x128\n",
    "        #output: 32x32x1\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN*2, G_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #out layer\n",
    "        #input: 32x32x64\n",
    "        #output: 32x32x1\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNELS, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #1st layer\n",
    "        #input: 32x32x1\n",
    "        #output: 16x16x64\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(IMAGE_CHANNELS, D_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #2nd layer\n",
    "        #input: 16x16x64\n",
    "        #output: 8x8x128\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #3rd layer\n",
    "        #input: 8x8x128\n",
    "        #output: 4x4x256\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #4th layer\n",
    "        #input: 4x4x256\n",
    "        #output: 2x2x512\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #out layer\n",
    "        #input: 2x2x512\n",
    "        #output: 1x1x1\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN * 8, 1, 2, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.out_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator and discriminator objects with the\n",
    "# loss function and optimizers.\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset object and dataloader object.\n",
    "dataset = dset.MNIST(root=DATA_PATH, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "]))\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4367, 0.4632, 0.5033],\n",
      "         [0.3937, 0.6706, 0.5046],\n",
      "         [0.5379, 0.4531, 0.5175]]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1, 3, 3])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanso\\Documents\\Uni\\TFG\\dcGAN\\dcgan.ipynb Celda 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/TFG/dcGAN/dcgan.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_real \u001b[39m=\u001b[39m discriminator(x_real)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/TFG/dcGAN/dcgan.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(y_real[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/TFG/dcGAN/dcgan.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss_D_real \u001b[39m=\u001b[39m loss_func(y_real, real_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/TFG/dcGAN/dcgan.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss_D_real\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/TFG/dcGAN/dcgan.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#Now train with fake data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3084\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3085\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3089\u001b[0m     )\n\u001b[0;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1, 3, 3])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "#Train loop\n",
    "\n",
    "viz_noise = torch.randn(BATCH_SIZE, RANDOM_DIM, 1, 1, device=device)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x_real = data[0].to(device)\n",
    "        real_label = torch.full((x_real.size(0),), REAL_LABEL, device=device)\n",
    "        fake_label = torch.full((x_real.size(0),), FAKE_LABEL, device=device)\n",
    "\n",
    "        # Train the discriminator.\n",
    "        #First train with real data.\n",
    "        discriminator.zero_grad()\n",
    "        y_real = discriminator(x_real)\n",
    "        print(y_real[0])\n",
    "        loss_D_real = loss_func(y_real, real_label)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "        #Now train with fake data.\n",
    "        z_noise = torch.randn(x_real.size(0), RANDOM_DIM, 1, 1, device=device)\n",
    "        x_fake = generator(z_noise)\n",
    "        y_fake = discriminator(x_fake.detach())\n",
    "        loss_D_fake = loss_func(y_fake, fake_label)\n",
    "        loss_D_fake.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Train the generator.\n",
    "        generator.zero_grad()\n",
    "        y_fake = discriminator(x_fake)\n",
    "        loss_G = loss_func(y_fake, real_label)\n",
    "        loss_G.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch {} [{}/{}] loss_D_real: {:.4f} loss_D_fake:{:.4f} loss_G: {:.4f}'.format(\n",
    "                epoch, i, len(dataloader),\n",
    "                loss_D_real.mean().item(),\n",
    "                loss_D_fake.mean().item(),\n",
    "                loss_G.mean().item()))\n",
    "            # Save the generated images from this iteration\n",
    "            vutils.save_image(x_real, '{}/real_samples.png'.format(OUT_PATH), normalize=True)\n",
    "            with torch.no_grad():\n",
    "                viz_sample = generator(viz_noise)\n",
    "                vutils.save_image(viz_sample, '{}/fake_samples_epoch_{:03d}.png'.format(OUT_PATH, epoch), normalize=True)\n",
    "                torch.save(generator.state_dict(), '{}/generator_epoch_{:03d}.pth'.format(OUT_PATH, epoch))\n",
    "                torch.save(discriminator.state_dict(), '{}/discriminator_epoch_{:03d}.pth'.format(OUT_PATH, epoch))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torchCUDA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7686141916f2a7fb95be6dc9b0aff42fd285a1fe1b5d9e9cd4ffc7bf000df46c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
