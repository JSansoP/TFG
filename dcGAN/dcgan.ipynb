{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "\n",
    "CUDA = True\n",
    "DATA_PATH = 'data'\n",
    "OUT_PATH = 'out'\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_CHANNELS = 1\n",
    "IMAGE_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "REAL_LABEL = 1\n",
    "FAKE_LABEL = 0\n",
    "LEARNING_RATE = 0.0002\n",
    "SEED = 1\n",
    "RANDOM_DIM = 100\n",
    "G_HIDDEN = 64\n",
    "D_HIDDEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dset.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True . Pytorch version:  1.13.0\n",
      "Seed: 1\n"
     ]
    }
   ],
   "source": [
    "#If output folder doesn't exist, create it. Else, delete all files in it.\n",
    "if not os.path.exists(OUT_PATH):\n",
    "    os.makedirs(OUT_PATH)\n",
    "else:\n",
    "    for f in os.listdir(OUT_PATH):\n",
    "        os.remove(os.path.join(OUT_PATH, f))\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "print(\"CUDA Available: \", CUDA, \". Pytorch version: \", torch.__version__)\n",
    "print(\"Seed:\", SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #1st layer\n",
    "        #input: 100x1x1\n",
    "        #output: 512x4x4 bc kernel size is 4 and stride is 1,\n",
    "        # so output size is (input size - 1) * stride + kernel size.\n",
    "        # (1 - 1) * 1 + 4 = 4\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(RANDOM_DIM, G_HIDDEN * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #2nd layer\n",
    "        #input: 512x4x4\n",
    "        #output: 256x8x8\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #3rd layer\n",
    "        #input: 256x8x8\n",
    "        #output: 128x16x16\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #4th layer\n",
    "        #input: 128x16x16\n",
    "        #output: 64x32x32\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        #5th layer\n",
    "        #input: 64x32x32\n",
    "        #output: 1x64x64\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNELS, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.out_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #1st layer\n",
    "        #input: 64x64x1\n",
    "        #output: 32x32x64\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(IMAGE_CHANNELS, D_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #2nd layer\n",
    "        #input: 32x32x64\n",
    "        #output: 16x16x128\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #3rd layer\n",
    "        #input: 16x16x128\n",
    "        #output: 8x8x256\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #4th layer\n",
    "        #input: 8x8x256\n",
    "        #output: 4x4x512\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        #out layer\n",
    "        #input: 4x4x512\n",
    "        #output: 1x1x1\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Conv2d(D_HIDDEN * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.out_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator and discriminator objects with the\n",
    "# loss function and optimizers.\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset object and dataloader object.\n",
    "dataset = dset.MNIST(root=DATA_PATH, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "]))\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [0/469] loss_D_real: 0.3184 loss_D_fake:0.1684 loss_G: 2.4040\n",
      "Epoch 0 [100/469] loss_D_real: 0.3656 loss_D_fake:0.0713 loss_G: 1.5529\n",
      "Epoch 0 [200/469] loss_D_real: 0.1442 loss_D_fake:0.6806 loss_G: 1.6267\n",
      "Epoch 0 [300/469] loss_D_real: 1.0420 loss_D_fake:0.0517 loss_G: 0.3723\n",
      "Epoch 0 [400/469] loss_D_real: 0.6705 loss_D_fake:0.0648 loss_G: 0.9549\n",
      "Epoch 1 [0/469] loss_D_real: 0.1472 loss_D_fake:0.1057 loss_G: 2.2515\n",
      "Epoch 1 [100/469] loss_D_real: 0.3162 loss_D_fake:0.1620 loss_G: 1.8530\n",
      "Epoch 1 [200/469] loss_D_real: 0.4851 loss_D_fake:0.2231 loss_G: 1.7614\n",
      "Epoch 1 [300/469] loss_D_real: 0.0448 loss_D_fake:0.5529 loss_G: 4.5356\n",
      "Epoch 1 [400/469] loss_D_real: 0.2436 loss_D_fake:0.2128 loss_G: 2.1988\n",
      "Epoch 2 [0/469] loss_D_real: 0.6014 loss_D_fake:0.1712 loss_G: 1.1329\n",
      "Epoch 2 [100/469] loss_D_real: 0.0798 loss_D_fake:0.3599 loss_G: 3.2430\n",
      "Epoch 2 [200/469] loss_D_real: 0.0852 loss_D_fake:0.0249 loss_G: 4.0708\n",
      "Epoch 2 [300/469] loss_D_real: 0.0848 loss_D_fake:0.2175 loss_G: 3.7183\n",
      "Epoch 2 [400/469] loss_D_real: 0.0688 loss_D_fake:0.6203 loss_G: 3.0356\n",
      "Epoch 3 [0/469] loss_D_real: 0.7739 loss_D_fake:0.1245 loss_G: 1.2017\n",
      "Epoch 3 [100/469] loss_D_real: 0.0197 loss_D_fake:0.0127 loss_G: 4.5189\n",
      "Epoch 3 [200/469] loss_D_real: 0.8707 loss_D_fake:0.0289 loss_G: 1.2301\n",
      "Epoch 3 [300/469] loss_D_real: 0.3318 loss_D_fake:0.0616 loss_G: 1.9416\n",
      "Epoch 3 [400/469] loss_D_real: 0.1563 loss_D_fake:0.2444 loss_G: 2.2188\n",
      "Epoch 4 [0/469] loss_D_real: 0.0303 loss_D_fake:0.0555 loss_G: 4.4797\n",
      "Epoch 4 [100/469] loss_D_real: 0.1476 loss_D_fake:0.1928 loss_G: 3.1532\n",
      "Epoch 4 [200/469] loss_D_real: 0.0388 loss_D_fake:0.0232 loss_G: 4.2669\n",
      "Epoch 4 [300/469] loss_D_real: 0.0637 loss_D_fake:0.8264 loss_G: 4.1982\n",
      "Epoch 4 [400/469] loss_D_real: 0.0239 loss_D_fake:0.5966 loss_G: 4.9345\n",
      "Epoch 5 [0/469] loss_D_real: 0.1814 loss_D_fake:0.4570 loss_G: 3.6812\n",
      "Epoch 5 [100/469] loss_D_real: 0.0014 loss_D_fake:0.0000 loss_G: 18.5567\n",
      "Epoch 5 [200/469] loss_D_real: 0.0009 loss_D_fake:0.0012 loss_G: 7.9660\n",
      "Epoch 5 [300/469] loss_D_real: 0.0004 loss_D_fake:0.0011 loss_G: 7.5307\n",
      "Epoch 5 [400/469] loss_D_real: 0.0005 loss_D_fake:0.0005 loss_G: 7.8486\n",
      "Epoch 6 [0/469] loss_D_real: 0.0007 loss_D_fake:0.0007 loss_G: 8.0265\n",
      "Epoch 6 [100/469] loss_D_real: 0.0650 loss_D_fake:0.0000 loss_G: 18.1747\n",
      "Epoch 6 [200/469] loss_D_real: 0.0714 loss_D_fake:0.0990 loss_G: 4.7974\n",
      "Epoch 6 [300/469] loss_D_real: 0.0485 loss_D_fake:0.0506 loss_G: 3.9574\n",
      "Epoch 6 [400/469] loss_D_real: 0.0023 loss_D_fake:2.2711 loss_G: 15.3124\n",
      "Epoch 7 [0/469] loss_D_real: 0.0765 loss_D_fake:0.1014 loss_G: 3.7401\n",
      "Epoch 7 [100/469] loss_D_real: 0.2036 loss_D_fake:0.2406 loss_G: 2.0567\n",
      "Epoch 7 [200/469] loss_D_real: 0.0136 loss_D_fake:0.0491 loss_G: 4.0832\n",
      "Epoch 7 [300/469] loss_D_real: 0.0850 loss_D_fake:0.0459 loss_G: 3.0237\n",
      "Epoch 7 [400/469] loss_D_real: 0.1280 loss_D_fake:0.0623 loss_G: 4.0983\n",
      "Epoch 8 [0/469] loss_D_real: 0.0117 loss_D_fake:0.0789 loss_G: 5.6251\n",
      "Epoch 8 [100/469] loss_D_real: 0.0236 loss_D_fake:1.3277 loss_G: 4.1431\n",
      "Epoch 8 [200/469] loss_D_real: 0.1061 loss_D_fake:0.4079 loss_G: 3.6798\n",
      "Epoch 8 [300/469] loss_D_real: 0.7066 loss_D_fake:0.0972 loss_G: 1.3398\n",
      "Epoch 8 [400/469] loss_D_real: 0.0821 loss_D_fake:0.0519 loss_G: 3.2216\n",
      "Epoch 9 [0/469] loss_D_real: 0.0007 loss_D_fake:1.0526 loss_G: 15.5504\n",
      "Epoch 9 [100/469] loss_D_real: 0.0100 loss_D_fake:0.1113 loss_G: 5.0720\n",
      "Epoch 9 [200/469] loss_D_real: 0.0538 loss_D_fake:0.0356 loss_G: 3.6673\n",
      "Epoch 9 [300/469] loss_D_real: 0.1916 loss_D_fake:0.1467 loss_G: 2.3339\n",
      "Epoch 9 [400/469] loss_D_real: 0.0706 loss_D_fake:0.8091 loss_G: 5.1758\n",
      "Epoch 10 [0/469] loss_D_real: 0.1196 loss_D_fake:0.0877 loss_G: 3.1171\n",
      "Epoch 10 [100/469] loss_D_real: 0.0116 loss_D_fake:0.0423 loss_G: 5.0449\n",
      "Epoch 10 [200/469] loss_D_real: 0.3504 loss_D_fake:0.0974 loss_G: 2.1580\n",
      "Epoch 10 [300/469] loss_D_real: 0.0432 loss_D_fake:0.0352 loss_G: 3.8964\n",
      "Epoch 10 [400/469] loss_D_real: 0.0256 loss_D_fake:0.1860 loss_G: 5.2568\n",
      "Epoch 11 [0/469] loss_D_real: 0.0195 loss_D_fake:0.0093 loss_G: 4.7407\n",
      "Epoch 11 [100/469] loss_D_real: 0.1662 loss_D_fake:0.1213 loss_G: 2.3125\n",
      "Epoch 11 [200/469] loss_D_real: 0.4684 loss_D_fake:0.0543 loss_G: 1.8237\n",
      "Epoch 11 [300/469] loss_D_real: 0.0558 loss_D_fake:0.1095 loss_G: 3.5527\n",
      "Epoch 11 [400/469] loss_D_real: 0.0246 loss_D_fake:0.0324 loss_G: 4.4251\n",
      "Epoch 12 [0/469] loss_D_real: 0.0074 loss_D_fake:0.0330 loss_G: 5.1858\n",
      "Epoch 12 [100/469] loss_D_real: 0.1432 loss_D_fake:0.1104 loss_G: 2.5133\n",
      "Epoch 12 [200/469] loss_D_real: 0.8519 loss_D_fake:0.1390 loss_G: 0.1309\n",
      "Epoch 12 [300/469] loss_D_real: 0.0050 loss_D_fake:0.0105 loss_G: 5.4532\n",
      "Epoch 12 [400/469] loss_D_real: 0.3725 loss_D_fake:0.3338 loss_G: 2.1557\n",
      "Epoch 13 [0/469] loss_D_real: 0.1173 loss_D_fake:0.4338 loss_G: 2.1084\n",
      "Epoch 13 [100/469] loss_D_real: 0.3414 loss_D_fake:0.3116 loss_G: 2.4320\n",
      "Epoch 13 [200/469] loss_D_real: 0.3606 loss_D_fake:0.2471 loss_G: 1.9382\n",
      "Epoch 13 [300/469] loss_D_real: 0.0170 loss_D_fake:0.0173 loss_G: 4.5877\n",
      "Epoch 13 [400/469] loss_D_real: 0.0081 loss_D_fake:0.0085 loss_G: 5.3602\n",
      "Epoch 14 [0/469] loss_D_real: 0.0148 loss_D_fake:0.0085 loss_G: 4.8373\n",
      "Epoch 14 [100/469] loss_D_real: 0.0774 loss_D_fake:0.1541 loss_G: 3.1287\n",
      "Epoch 14 [200/469] loss_D_real: 0.1674 loss_D_fake:0.0857 loss_G: 2.7549\n",
      "Epoch 14 [300/469] loss_D_real: 0.0313 loss_D_fake:0.0597 loss_G: 4.1886\n",
      "Epoch 14 [400/469] loss_D_real: 0.0191 loss_D_fake:0.0371 loss_G: 5.1386\n",
      "Epoch 15 [0/469] loss_D_real: 0.0108 loss_D_fake:0.3990 loss_G: 6.3128\n",
      "Epoch 15 [100/469] loss_D_real: 0.0214 loss_D_fake:0.0119 loss_G: 4.6762\n",
      "Epoch 15 [200/469] loss_D_real: 0.0178 loss_D_fake:0.0063 loss_G: 4.4882\n",
      "Epoch 15 [300/469] loss_D_real: 0.0102 loss_D_fake:0.0048 loss_G: 5.5195\n",
      "Epoch 15 [400/469] loss_D_real: 0.4224 loss_D_fake:0.4121 loss_G: 1.5280\n",
      "Epoch 16 [0/469] loss_D_real: 0.2999 loss_D_fake:0.2214 loss_G: 2.8026\n",
      "Epoch 16 [100/469] loss_D_real: 0.8605 loss_D_fake:0.0783 loss_G: 0.7759\n",
      "Epoch 16 [200/469] loss_D_real: 0.0610 loss_D_fake:0.0946 loss_G: 3.6254\n",
      "Epoch 16 [300/469] loss_D_real: 0.0929 loss_D_fake:0.0876 loss_G: 3.8496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\PIL\\ImageFile.py:495\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[0;32m    496\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanso\\Documents\\Uni\\4t\\TFG\\dcGAN\\dcgan.ipynb Celda 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/4t/TFG/dcGAN/dcgan.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/4t/TFG/dcGAN/dcgan.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     viz_sample \u001b[39m=\u001b[39m generator(viz_noise)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/4t/TFG/dcGAN/dcgan.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     vutils\u001b[39m.\u001b[39;49msave_image(viz_sample, \u001b[39m'\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/fake_samples_epoch_\u001b[39;49m\u001b[39m{:03d}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(OUT_PATH, epoch), normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/4t/TFG/dcGAN/dcgan.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(generator\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/generator_epoch_\u001b[39m\u001b[39m{:03d}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(OUT_PATH, epoch))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanso/Documents/Uni/4t/TFG/dcGAN/dcgan.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(discriminator\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/discriminator_epoch_\u001b[39m\u001b[39m{:03d}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(OUT_PATH, epoch))\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\torchvision\\utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    150\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[1;32m--> 151\u001b[0m im\u001b[39m.\u001b[39;49msave(fp, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\PIL\\Image.py:2212\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2209\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2212\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[0;32m   2213\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2214\u001b[0m     \u001b[39m# do what we can to clean up\u001b[39;00m\n\u001b[0;32m   2215\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\PIL\\PngImagePlugin.py:1348\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[0;32m   1347\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[0;32m   1350\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[0;32m   1351\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\PIL\\ImageFile.py:510\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     l, s, d \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mencode(bufsize)\n\u001b[1;32m--> 510\u001b[0m     fp\u001b[39m.\u001b[39;49mwrite(d)\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m s:\n\u001b[0;32m    512\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\PIL\\PngImagePlugin.py:1044\u001b[0m, in \u001b[0;36m_idat.write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m-> 1044\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mIDAT\u001b[39;49m\u001b[39m\"\u001b[39;49m, data)\n",
      "File \u001b[1;32mc:\\Users\\sanso\\miniconda3\\envs\\torchCUDA\\lib\\site-packages\\PIL\\PngImagePlugin.py:1031\u001b[0m, in \u001b[0;36mputchunk\u001b[1;34m(fp, cid, *data)\u001b[0m\n\u001b[0;32m   1028\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(data)\n\u001b[0;32m   1030\u001b[0m fp\u001b[39m.\u001b[39mwrite(o32(\u001b[39mlen\u001b[39m(data)) \u001b[39m+\u001b[39m cid)\n\u001b[1;32m-> 1031\u001b[0m fp\u001b[39m.\u001b[39;49mwrite(data)\n\u001b[0;32m   1032\u001b[0m crc \u001b[39m=\u001b[39m _crc32(data, _crc32(cid))\n\u001b[0;32m   1033\u001b[0m fp\u001b[39m.\u001b[39mwrite(o32(crc))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train loop\n",
    "\n",
    "viz_noise = torch.randn(BATCH_SIZE, RANDOM_DIM, 1, 1, device=device)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x_real = data[0].to(device)\n",
    "        real_label = torch.full((x_real.size(0),), REAL_LABEL, device=device).float()\n",
    "        fake_label = torch.full((x_real.size(0),), FAKE_LABEL, device=device).float()\n",
    "\n",
    "        # Train the discriminator.\n",
    "        #First train with real data.\n",
    "        discriminator.zero_grad()\n",
    "        y_real = discriminator(x_real).view(-1)\n",
    "        loss_D_real = loss_func(y_real, real_label)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "        #Now train with fake data.\n",
    "        z_noise = torch.randn(x_real.size(0), RANDOM_DIM, 1, 1, device=device)\n",
    "        x_fake = generator(z_noise)\n",
    "        y_fake = discriminator(x_fake.detach()).view(-1)\n",
    "        loss_D_fake = loss_func(y_fake, fake_label)\n",
    "        loss_D_fake.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Train the generator.\n",
    "        generator.zero_grad()\n",
    "        y_fake = discriminator(x_fake).view(-1)\n",
    "        loss_G = loss_func(y_fake, real_label)\n",
    "        loss_G.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch {} [{}/{}] loss_D_real: {:.4f} loss_D_fake:{:.4f} loss_G: {:.4f}'.format(\n",
    "                epoch, i, len(dataloader),\n",
    "                loss_D_real.mean().item(),\n",
    "                loss_D_fake.mean().item(),\n",
    "                loss_G.mean().item()))\n",
    "            # Save the generated images from this iteration\n",
    "            vutils.save_image(x_real, '{}/real_samples.png'.format(OUT_PATH), normalize=True)\n",
    "            with torch.no_grad():\n",
    "                viz_sample = generator(viz_noise)\n",
    "                vutils.save_image(viz_sample, '{}/fake_samples_epoch_{:03d}.png'.format(OUT_PATH, epoch), normalize=True)\n",
    "                torch.save(generator.state_dict(), '{}/generator_epoch_{:03d}.pth'.format(OUT_PATH, epoch))\n",
    "                torch.save(discriminator.state_dict(), '{}/discriminator_epoch_{:03d}.pth'.format(OUT_PATH, epoch))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torchCUDA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7686141916f2a7fb95be6dc9b0aff42fd285a1fe1b5d9e9cd4ffc7bf000df46c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
